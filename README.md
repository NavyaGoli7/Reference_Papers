# Reference_Papers
Related to efficient training/fine-tuning design for LLMs

1. Zhang, Y., Liu, Y., Yuan, H., Qin, Z., Yuan, Y., Gu, Q., & Yao, A. C. C. (2025). Tensor product attention is all you need. arXiv preprint [arXiv:2501.06425](https://arxiv.org/pdf/2501.06425)
- The main goal of TPA is to address the severe memory overhead caused by Key-Value (KV) caches during autoregressive inference. This is crucial for enabling LLMs to process longer context sequences.
2. Yang, X., Leng, J., Guo, G., Zhao, J., Nakada, R., Zhang, L., ... & Chen, B. (2024). S $^{2} $ FT: Efficient, scalable and generalizable LLM fine-tuning by structured sparsity. Advances in Neural Information Processing Systems, 37, 59912-59947.[paper link](https://proceedings.neurips.cc/paper_files/paper/2024/file/6e3b9fb0c0c56cf6e1ee61e6a068fca4-Paper-Conference.pdf)
  - The fundamental contribution of S2FT is its ability to concurrently achieve state-of-the-art (SOTA) fine-tuning performance, training efficiency, and inference scalability
3. Li, K., Han, S., Su, Q., Li, W., Cai, Z., & Ji, S. (2025). Uni-LoRA: One Vector is All You Need. arXiv preprint [arXiv:2506.00799](https://arxiv.org/pdf/2506.00799)
- Uni-LoRA's specific "one-vector-only" solution uses an efficient, fixed, and isometric projection matrix that enables global parameter sharing and requires training only this single vector θ\_d to reconstruct parameters for the entire LLM, achieving state-of-the-art parameter efficiency with O(D) time complexity
4. Jeon, H., Lee, S., Kang, B., Kim, Y., & Kim, J. J. (2025). QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models. [arXiv preprint arXiv:2509.17428](https://arxiv.org/pdf/2509.17428)
  - QWHA contributes to efficient LLM design by offering a PEFT method that is both computationally efficient during training (due to the WHA design) and effective at preserving accuracy in highly compressed, efficient LLMs for inference (due to its QA initialization scheme)
5. Deng, Y., Zhang, A., Gurses, S., Wang, N., Yang, Z., & Yin, P. (2025). Cloq: Enhancing fine-tuning of quantized llms via calibrated lora initialization. arXiv preprint [arXiv:2501.18475](https://arxiv.org/pdf/2501.18475)
- CLoQ is an efficient design because it focuses on minimizing the computational cost of the initialization step while maximizing the final performance of the memory-efficient combination of LoRA and quantization, a critical area for efficient LLM design
6. Wang, H., Ma, S., Ma, L., Wang, L., Wang, W., Dong, L., ... & Wei, F. (2025). Bitnet: 1-bit pre-training for large language models. Journal of Machine Learning Research, 26(125), 1-29. [paper link](https://arxiv.org/pdf/2402.17764)
- The paper introduces BitNet b1.58, a 1.58-bit LLM variant where every parameter (weight) is ternary {−1,0,1}. The core innovation is to make LLMs significantly more cost-effective and efficient, especially during inference.
7. Hsu, C. Y., Tsai, Y. L., Lin, C. H., Chen, P. Y., Yu, C. M., & Huang, C. Y. (2024). Safe lora: The silver lining of reducing safety risks when finetuning large language models. Advances in Neural Information Processing Systems, 37, 65072-65094. [paper link](https://proceedings.neurips.cc/paper_files/paper/2024/file/77baa7c2a3a675823e89131698fd6e19-Paper-Conference.pdf)
  - The work leverages an existing efficient fine-tuning method (LoRA) and introduces a mechanism to address a critical flaw (safety degradation) without introducing significant computational overhead, thereby making the process of fine-tuning more cost-effective and reliable.
8. Yu, Z., Wang, Z., Li, Y., Gao, R., Zhou, X., Bommu, S. R., ... & Lin, Y. (2024, June). Edge-llm: Enabling efficient large language model adaptation on edge devices via unified compression and adaptive layer voting. In Proceedings of the 61st ACM/IEEE Design Automation Conference (pp. 1-6). [paper link](http://dl.acm.org/doi/pdf/10.1145/3649329.3658473)
  
9. Wu, Y., Li, J., Tian, C., Guo, Z., & Li, L. (2025). Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning. arXiv preprint [arXiv:2508.17209](https://arxiv.org/pdf/2508.17209)
  - The core contribution of this work is introducing a paradigm for memory-efficient federated fine-tuning of LLMs, specifically designed to overcome severe memory limitations on resource-constrained devices.
10. Gao, L., Ziashahabi, A., Niu, Y., Avestimehr, S., & Annavaram, M. (2025, November). MobiZO: Enabling efficient llm fine-tuning at the edge via inference engines. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (pp. 20217-20234).[paper link] (https://aclanthology.org/2025.emnlp-main.1022.pdf)
  - This paper describes an efficient fine-tuning design that achieves efficiency by adopting ZO optimization (to save memory) and then introducing novel parallelization techniques (MP-LoRA, outer-loop, and inner-loop) to dramatically increase the computation speed and effectiveness of ZO optimization on resource-constrained edge devices
11. Mohanty, A., Kang, G., Gao, L., & Annavaram, M. (2025). DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge. arXiv preprint [arXiv:2510.16716](https://arxiv.org/pdf/2510.16716)
  - The work tackles the challenge of enabling LLM personalization by fine-tuning models directly on edge devices, a process made possible by the growth of edge-centric ML accelerators and resource-efficient training algorithms


6 Dec,2025:

12. Gogineni, K., Suvizi, A., & Venkataramani, G. (2025). Llms on a budget: System-level approaches to power-efficient and scalable fine-tuning. IEEE Open Journal of the Computer Society. (paper link)[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11037824]
  - The study investigates system-level optimizations like activation checkpointing, low-rank adaptation (LoRA), and operation fusion. Using all optimizations reduced memory usage by over 40% compared to FP32 baselines.The work also analyzes GPU peak power capping during the fine-tuning phase,. Capping the GPU power to 300 W reduced power consumption by 33%.

 13. Gao, L., Ziashahabi, A., Niu, Y., Avestimehr, S., & Annavaram, M. (2025, November). MobiZO: Enabling efficient llm fine-tuning at the edge via inference engines. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (pp. 20217-20234). (paper link)[https://aclanthology.org/2025.emnlp-main.1022.pdf]
  - MobiZO, a resource-efficient fine-tuning framework for LLMs specifically designed for edge devices. MobiZO combines three key innovations: (1) a parallelized randomized gradient estimator that employs both outer-loop and innerloop parallelism to eliminate sequential forward passes, (2) a specialized Multi-Perturbed LoRA (MP-LoRA) module that enables efficient realization of both inner and outer loop parallelism, and (3) a seamless integration with ExecuTorch for on-device training, requiring no modifications to the runtime.

 14. Khaki, S., Li, X., Guo, J., Zhu, L., Xu, C., Plataniotis, K. N., ... & Liu, Z. (2025). SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity. arXiv preprint (arXiv:2506.1650)[https://arxiv.org/pdf/2506.16500]
     -  SparseLoRA reduces computational cost by up to 2.2× and a measured speedup of up to 1.6× while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following. SparseLoRA, a method that accelerates LLM finetuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps.

15. Lin, X., Wang, W., Li, Y., Yang, S., Feng, F., Wei, Y., & Chua, T. S. (2024, July). Data-efficient Fine-tuning for LLM-based Recommendation. In Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval (pp. 365-374).(paper link)[https://dl.acm.org/doi/pdf/10.1145/3626772.3657807]
- The paper addresses the data pruning task for efficient LLM-based recommendation by identifying a small, representative subset of samples for few-shot fine-tuning. The proposed method, DEALRec, uses an influence score and an effort score (gap regularization) to efficiently select samples that lead to high accuracy. Empirically, DEALRec uses only 2% of the training data to surpass full data fine-tuning while reducing time costs by over 97% on average.
16. Tian, C., Shi, Z., Guo, Z., Li, L., & Xu, C. Z. (2024). Hydralora: An asymmetric lora architecture for efficient fine-tuning. Advances in Neural Information Processing Systems, 37, 9565-9584. (paper link) []
  - HydraLoRA consistently outperforms standard LoRA and other PEFT baselines across single and multi-task domains, often exceeding the performance of methods with similar or larger parameter counts. In terms of system efficiency, HydraLoRA significantly speeds up the training process by 1.96x compared to LoRA (rank=32). This structural efficiency results in a 49.6% reduction in energy consumption during fine-tuning.
 17. Bian, J., Wang, L., Zhang, L., & Xu, J. (2025). Lora-fair: Federated lora fine-tuning with aggregation and initialization refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3737-3746). (paper link)[https://openaccess.thecvf.com/content/ICCV2025/html/Bian_LoRA-FAIR_Federated_LoRA_Fine-Tuning_with_Aggregation_and_Initialization_Refinement_ICCV_2025_paper.html]
 - LoRA-FAIR is a novel method designed for federated fine-tuning using LoRA that simultaneously addresses Server-Side Aggregation Bias and Client-Side Initialization Lag. It achieves this by introducing a residual correction term (ΔB) on the server to refine the aggregated LoRA modules, yielding superior performance and maintaining computational efficiency.
