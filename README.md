# Reference_Papers
Related to efficient training/fine-tuning design for LLMs

1. Zhang, Y., Liu, Y., Yuan, H., Qin, Z., Yuan, Y., Gu, Q., & Yao, A. C. C. (2025). Tensor product attention is all you need. arXiv preprint [arXiv:2501.06425](https://arxiv.org/pdf/2501.06425)
- The main goal of TPA is to address the severe memory overhead caused by Key-Value (KV) caches during autoregressive inference. This is crucial for enabling LLMs to process longer context sequences.
2. Yang, X., Leng, J., Guo, G., Zhao, J., Nakada, R., Zhang, L., ... & Chen, B. (2024). S $^{2} $ FT: Efficient, scalable and generalizable LLM fine-tuning by structured sparsity. Advances in Neural Information Processing Systems, 37, 59912-59947.[paper link](https://proceedings.neurips.cc/paper_files/paper/2024/file/6e3b9fb0c0c56cf6e1ee61e6a068fca4-Paper-Conference.pdf)
  - The fundamental contribution of S2FT is its ability to concurrently achieve state-of-the-art (SOTA) fine-tuning performance, training efficiency, and inference scalability
3. Li, K., Han, S., Su, Q., Li, W., Cai, Z., & Ji, S. (2025). Uni-LoRA: One Vector is All You Need. arXiv preprint [arXiv:2506.00799](https://arxiv.org/pdf/2506.00799)
- Uni-LoRA's specific "one-vector-only" solution uses an efficient, fixed, and isometric projection matrix that enables global parameter sharing and requires training only this single vector θ\_d to reconstruct parameters for the entire LLM, achieving state-of-the-art parameter efficiency with O(D) time complexity
4. Jeon, H., Lee, S., Kang, B., Kim, Y., & Kim, J. J. (2025). QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models. [arXiv preprint arXiv:2509.17428](https://arxiv.org/pdf/2509.17428)
  - QWHA contributes to efficient LLM design by offering a PEFT method that is both computationally efficient during training (due to the WHA design) and effective at preserving accuracy in highly compressed, efficient LLMs for inference (due to its QA initialization scheme)
5. Deng, Y., Zhang, A., Gurses, S., Wang, N., Yang, Z., & Yin, P. (2025). Cloq: Enhancing fine-tuning of quantized llms via calibrated lora initialization. arXiv preprint [arXiv:2501.18475](https://arxiv.org/pdf/2501.18475)
- CLoQ is an efficient design because it focuses on minimizing the computational cost of the initialization step while maximizing the final performance of the memory-efficient combination of LoRA and quantization, a critical area for efficient LLM design
6. Wang, H., Ma, S., Ma, L., Wang, L., Wang, W., Dong, L., ... & Wei, F. (2025). Bitnet: 1-bit pre-training for large language models. Journal of Machine Learning Research, 26(125), 1-29. [paper link](https://arxiv.org/pdf/2402.17764)
- The paper introduces BitNet b1.58, a 1.58-bit LLM variant where every parameter (weight) is ternary {−1,0,1}. The core innovation is to make LLMs significantly more cost-effective and efficient, especially during inference.
7. Hsu, C. Y., Tsai, Y. L., Lin, C. H., Chen, P. Y., Yu, C. M., & Huang, C. Y. (2024). Safe lora: The silver lining of reducing safety risks when finetuning large language models. Advances in Neural Information Processing Systems, 37, 65072-65094. [paper link](https://proceedings.neurips.cc/paper_files/paper/2024/file/77baa7c2a3a675823e89131698fd6e19-Paper-Conference.pdf)
  - The work leverages an existing efficient fine-tuning method (LoRA) and introduces a mechanism to address a critical flaw (safety degradation) without introducing significant computational overhead, thereby making the process of fine-tuning more cost-effective and reliable.
8. Yu, Z., Wang, Z., Li, Y., Gao, R., Zhou, X., Bommu, S. R., ... & Lin, Y. (2024, June). Edge-llm: Enabling efficient large language model adaptation on edge devices via unified compression and adaptive layer voting. In Proceedings of the 61st ACM/IEEE Design Automation Conference (pp. 1-6). [paper link](http://dl.acm.org/doi/pdf/10.1145/3649329.3658473)
  
9. Wu, Y., Li, J., Tian, C., Guo, Z., & Li, L. (2025). Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning. arXiv preprint [arXiv:2508.17209](https://arxiv.org/pdf/2508.17209)
  - The core contribution of this work is introducing a paradigm for memory-efficient federated fine-tuning of LLMs, specifically designed to overcome severe memory limitations on resource-constrained devices.
10. Gao, L., Ziashahabi, A., Niu, Y., Avestimehr, S., & Annavaram, M. (2025, November). MobiZO: Enabling efficient llm fine-tuning at the edge via inference engines. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (pp. 20217-20234).[paper link] (https://aclanthology.org/2025.emnlp-main.1022.pdf)
  - This paper describes an efficient fine-tuning design that achieves efficiency by adopting ZO optimization (to save memory) and then introducing novel parallelization techniques (MP-LoRA, outer-loop, and inner-loop) to dramatically increase the computation speed and effectiveness of ZO optimization on resource-constrained edge devices
11. Mohanty, A., Kang, G., Gao, L., & Annavaram, M. (2025). DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge. arXiv preprint [arXiv:2510.16716](https://arxiv.org/pdf/2510.16716)
  - The work tackles the challenge of enabling LLM personalization by fine-tuning models directly on edge devices, a process made possible by the growth of edge-centric ML accelerators and resource-efficient training algorithms
